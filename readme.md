# 前言
利用强化学习实现了俩个简单的例子，用到了pytorch，代码比较简单，配置好需要的库直接运行



# 一.cliffWalking

## （1）cliffWalking_by_random
导入gym的环境，让🐢乌龟随机的走，最后输出一下最终步数
## （2）cliffWalking_by_Sarsa
跟（1）比较起来，使用了Sarsa算法来构建Q表格，让乌龟🐢的移动有了方向（通过每个状态的价值来决定）
，但是由于它是一种比较保守的策略，导致靠近悬崖的路的价值也比较低，不能达到最优解。
## （3）cliffWalking_by_Q_learning
更（2）比较起来，只是在更新时候选择了价值最大的那个一个，这样可以达到最优解。

# 二.CartPole

## （1）test
最基本的实现，把Q表格用Q函数代替，通过神经网络来训练这个函数
## （2）reply_buffer
在（1）的基础上，增加了缓冲池，使得不是每次训练结束以后立马更新网络，而是隔一定次数再更新网络
## （3）fixed_Q_target 
在（2）的基础上把Q函数分成了预测函数和目标函数，各几轮同步，相当于把目标固定了，减少了网络训练复杂度
## （4）epsilon_decay
在（3）的基础把，动态调节$\epsilon$的值，使得训练速度更块，效果更好